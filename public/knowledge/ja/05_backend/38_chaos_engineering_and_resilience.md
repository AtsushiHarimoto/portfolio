# 38. 烈火の中での生まれ変わり：破壊の神とカオスエンジニアリング (Chaos Engineering)

> **種類**: SRE サイト・リライアビリティ・エンジニアリングとフォールトトレラントアーキテクチャの指針
> **重点**: システムが巨大になればなるほど、「絶対に壊れないでくれ」と祈ることはできなくなります。それは非現実的な幻想です。本章では、Netflix が創始した狂気の哲学：**カオスエンジニアリング (Chaos Engineering)** を深く掘り下げます。本番環境に自らネットワークケーブルを引き抜く狂った猿の群れ (Chaos Monkey) を放ち、刀槍不入のフォールトトレラント (Fault Tolerance / 障害許容力) 体質を鍛え上げる方法を学びます。

---

## 序：防御で埋め尽くされたコードは、防御の成功を意味しない

これまでのアーキテクチャ設計の中で、私たちはマイクロサービスの**サーキットブレーカー (Circuit Breaker)**、Redis の **Active-Active (双活のクロスリージョン冗長化)**、そして Kubernetes の自動再起動 **Auto-Healing** メカニズムを導入してきました。

アーキテクチャ図は綺麗に描けましたが、あなたは胸を張ってこう保証できるでしょうか：
_「もし今日アメリカ東海岸のデータセンターが突然停電したとして、私たちが書いたルーティングは、本当に 2 秒以内に完璧にすべてのトラフィックを東京のデータセンターに引き継ぐことができるのだろうか？」_
_「もし決済 API が前兆もなく 30 秒にも及ぶ遅延を引き起こしたとして、私たちのサーキットブレーカーは本当に『パチン』と跳ね返り、この 30 秒間がトップページにいる 10 万人の訪問者すべてのスレッドプールをフリーズさせるのを防いでくれるのだろうか？」_

もし私たちがステージング環境 (Staging / テストサーバー) で、偽の緑色の交通信号を使ってテストしたつもりになっているだけなら、本当に災害が襲ってきた時、会社は間違いなく倒産します。
**唯一の解決策は、風が穏やかで日差しが暖かく、誰もがまだ起きて仕事をしている平日の昼間に、自ら核爆弾を起爆させることです！**

---

## 1. 狂気の自発的攻撃：カオスエンジニアリングの哲学

**カオスエンジニアリング (Chaos Engineering)** とは、単に破壊活動を行っているわけではありません。それは「システムが不安定な条件下において、**本番環境 (Production)** で制御された実験を行い、隠れた危機を早期に発見する」という科学です。

### 🦍 カオスモンキー (Chaos Monkey) を解き放て

Netflix が全社のビジネスを AWS クラウドに移行し始めた頃、彼らは痛烈な教訓を得ました。クラウドサーバーがいつ前触れもなくプラグを引っこ抜かれるか (EC2 Termination) と毎日ビクビク心配して過ごすくらいなら、いっそのこと自分たちで **Chaos Monkey (カオスモンキー)** という名のウイルスの小プログラムを書いてしまえ！と。

- **行動**：この猿は、平日のランダムな時間帯に、**何百万人の視聴者にドラマを配信している本番稼働環境のサーバーの電源ボタンを、生きたまま直接ポチッと切ってしまう (Kill) のです！**
- **重生**：猿が毎日プラグを引っこ抜いていくため、エンジニアはすべてのマイクロサービスをステートレス (Stateless) に書き直し、Kubernetes の自動補充メカニズムを強化せざるを得なくなりました。1 台のマシンが死ねば、即座に別のマシンが数秒の間に代わりを引き継ぎます。Netflix は業界で最も恐ろしい成果を達成しました：**「マシンは毎日死んでいるが、ユーザーは動画のセリフ 1 行たりともカクついたことがない。」**

---

## 2. カオスエンジニアリングの 4 段階の演習曲

いきなりデータベース全体をフォーマットしては（消去しては）いけません。それはただの犯罪です。制御されたカオス実験 (Chaos Experiment) は、厳格に 4 つの主要なステップに従わなければなりません：

1. **定常状態の確立 (Steady State)**：
   私たちは現在、ダッシュボード (Grafana/Datadog など) 上で何が「正常」なのかを定義しなければなりません。例：正常な状態では、トップページの読み込み遅延 (P99 Latency) は 100ms 未満であり、毎分の受注数は 500 件である。
2. **仮説の提示 (Hypothesis)**：
   ここで、災害が私たちを破滅させないことを祈り始めます：「私たちは 3 台のリレーショナルデータベースによるフォールトトレラントメカニズムを持っています。仮に私が今日、『そのうちの 1 台』のネットワークを強制的に切断したとしても、残りの 2 台がシームレスに引き継ぐはずです。トップページの遅延は依然として 100ms 未満にとどまり、注文数も正常な範囲から落ちることはないと**仮説**を立てます。」
3. **災害の注入 (Inject Faults)**：
   猿を放ちます！ Gremlin や Chaos Mesh のようなツールを使用します。
   - 決済マイクロサービスの CPU を強制的に 100% まで搾り取ります。
   - 故意に Redis を標的にして、500ms にも及ぶ遅延 (Network Delay) を注入します。
   - （さらに、大手企業の Chaos Gorilla クラスの巨大猿演習のように：ある地域全体の AZ データセンターの外部ネットワークを直接切断するようなことも行います）。
4. **検証または修復 (Verify or Abort)**：
   ダッシュボードを凝視します！もし注文数が暴落していなければ、おめでとうございます。あなたが設計したサーキットブレーカーとフォールトトレラントアーキテクチャ (Fault Tolerance) は本当に有効でした！
   しかし、もし大量の 500 Error を発見したなら、**直ちに「実験終了ボタン (Abort/Rollback)」を押して猿を回収し**、アーキテクチャの脆弱性の調査と修復に取り掛かります。午前 3 時にシステムが崩壊するのに比べれば、何百倍もマシです。

---

## 3. 防御的プログラミングの最高指導原則 (フォールトトレラント設計)

もしあなたが書いたマイクロサービスが、もうすぐこの狂った猿たちの洗礼を受けるとしたら、どのようにして雨宿りの準備 (未雨綢繆) をすべきでしょうか？

- **グレースフルデグラデーション (Graceful Degradation / 縮退運転)**：Amazon は「あなたへのおすすめ」の AI 推薦エンジンがダウンしたことを検知しても、トップページ全体を真っ白なエラー画面にすることはありません。彼らはプラン B を発動します。メモリ内にハードコーディングされた「過去の人気商品トップ 10」のリストを直接表示するのです。決済さえできれば、顧客は AI がダウンしたことなど全く気づきません。
- **タイムアウトとリトライの罠 (Timeout & Retry Storm)**：猿がある API を非常に遅くしてしまいました。必ずすべての API リクエストに厳格なタイムアウト (Timeout。例えば 2 秒) を設定してください。再試行 (Retry) する際は、1 秒間に何度も連打しては絶対にいけません。**指数バックオフ (Exponential Backoff、再試行の間隔を 1 秒、2 秒、4 秒...と広げる)** とランダムなブレ (Jitter) を必ず組み合わせてください。そうしないと、全サーバーのマイクロサービスが一斉に狂ったようにリトライを始め、さっき直ったばかりのデータベースを「身内のトラフィックによる DDoS の連続攻撃」で再び殴り殺してしまいます。

---

## 💡 Vibecoding 指令

AI Agent を使用して、バックエンドのマイクロサービス間で錯乱したネットワーク呼び出しや、高可用性システムアーキテクチャ図を構築する際：

> 🗣️ `「『外部の天気 API』や『内部の決済マイクロサービス』を呼び出す Client Wrapper を記述する際、何も考えずに直接 axios.get を使用することは厳禁です！私たちは SRE チームによる【カオスエンジニアリング (Chaos Engineering)】の極限の演習試練を受けようとしています。Resilience4j (または JS に対応するサーキットブレーカーパッケージ) を必ず導入し、この接続ポイントに最も厳格な【Timeout (タイムアウト切断)】と【Circuit Breaker (サーキットブレーカー)】の防衛線を掛けてください！さらに、【グレースフルデグラデーション (Fallback) 関数】を備えさせて安全なデフォルトのキャッシュデータを返すようにし、このクソ API がどれほど遅延しようとも、私たちのメインスレッドや Web ページの読み込み体験の足を引っ張らないように確実を期してください！」`
