# 07.3. ComfyUI 影像與語音生成模型採購指南

> **類型**: AI 模型選型與開源組件解析  
> **重點**: 針對 ComfyUI 此一節點式工作流引擎，系統化釐清 Checkpoint、LoRA、ControlNet 與多模態延伸套件之間的差異與定位。

---

若將 ComfyUI 生態系視為具備高度自動化的影音後製片廠，則各類具有 `.safetensors` 等特異副檔名之模型權重檔，即為受聘於廠內的不同職能大師。在此為 Moyin 專案梳理出各模組之專長領域與物理存放規範。

---

## 1. 核心圖像推理引擎 (Base Generation Models)

俗稱之「大模型」或「底模」，為依據文字提示詞 (Prompt) 建立畫面的核心職能角色。

| 模型層級 / 架構         | 技術特性與優勢                                                                                       | 領域適配建議                                                                             |
| :---------------------- | :--------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------- |
| **SDXL (性價比平衡型)** | 主流之擴散模型架構。具備極佳的開放社群生態，且對消費級顯卡 VRAM 之門檻要求適中。                     | 虛擬角色渲染、動漫風格繪製。強烈推薦 `Animagine XL 4.0` 或針對二次元微調之 `NoobAI XL`。 |
| **Flux (頂尖視覺質感)** | 次世代架構。能實現逼真的攝影級光影，更具備「精準拼寫圖中文字」之破格進化，惟運算記憶體消耗極其龐大。 | `Flux.1 Dev`：適用於商業級寫實素材與具字體排版需求之影像。                               |

> **📁 目錄部署指引**：上列 `.safetensors` 權重與結構檔，請全數安放於 `ComfyUI/models/checkpoints/` 目錄中。

---

## 2. 動態時序與影片生成模型 (Video Generation Models)

為靜止影像賦予物理流體運算或生命力之模型。現階於 Moyin 工作流體系中，我們強力依賴阿里巴巴釋出開源的高性能影片架構：**WAN 2.2 系列**。

| 任務種類        | 運作原理                                                                                  |
| :-------------- | :---------------------------------------------------------------------------------------- |
| **WAN 2.2 I2V** | Image-to-Video。以單張或多張靜態圖片作為種子 (Seed)，推演並渲染出後續幀之動態影片。       |
| **WAN 2.2 T2V** | Text-to-Video。純以文字提示詞做為輸入，透過擴散模型直接在時序空間中無中生有產出影像序列。 |

> 💡 **硬體負載舒緩對策 (Quantization)**：
> 影音生成模型之實體檔案通常極為龐大 (動輒 15GB 以上)。若遭遇顯卡 VRAM 過載或系統崩潰，請優先尋找社群發佈並標註有 **`FP8` (8-bit 浮點數)** 或 **`GGUF`** 之低精度量化壓縮版本下載。

---

## 3. 領域特化微調擴展 (LoRA - Low-Rank Adaptation)

**Low-Rank Adaptation (LoRA)** 係一種超輕量級之模型訓練手段。透過凍結主模型龐大之神經網路權重，僅透過極少數樣本 (如 20 張特定人物或畫風之圖片) 插入旁支運算層進行參數特訓。

- **價值**：產出的成品檔案極小 (約百來 MB 內)，可自由載入或熱抽換。用以迫使 AI 在生成影像時，精準鎖定並「模仿」指定的冷門角色五官、或套用特定之水彩/賽博朋克畫風。
- **📁 目錄部署指引**：請安放於 `ComfyUI/models/loras/` 目錄。

---

## 4. 零樣本語音克隆引擎 (Zero-Shot TTS)

文字轉換語音 (Text to Speech) 模組，係為虛擬系統提供動態生成之人聲。Moyin 取用以下最強開源解套方案：

### 🌟 GPT-SoVITS

為當前跨語種語音合成之霸主。

- **核心技術**：Zero-Shot (零樣本) 推論。僅需提供 3~5 秒之目標錄音檔做為參照音軌 (Reference Audio)，系統即可當場解析聲紋共鳴。
- **優勢**：完美兼容中、英、日、韓及粵語等多語系混抽，且可無縫於 ComfyUI Manager 內抓取與掛載。

### 🌟 IndexTTS2 (由社群 Bilibili 團隊開發)

- **技術偏重**：雖然部署難度略高，但其於「絕對影片聲軌時長控制（利於後期之嘴型 Lip-Sync 對接）」以及「高複雜度情緒解析（支援精確的語氣標籤置入）」表現卓絕。

---

## 5. 畫面精確性控制與輔助節點 (ControlNet & Consistency)

大語言模型最大的不可控性在於畫面的隨機漂移。以下輔助神經網路 (ControlNets) 將作為強力的空間或風格拘束器：

| 輔助模型類型              | 工程用途與技術效果                                                                                                                   |
| :------------------------ | :----------------------------------------------------------------------------------------------------------------------------------- |
| **ControlNet (OpenPose)** | 姿態骨架錨定器。餵入特製的骨架點圖或火柴人草稿，強制限縮後端擴散模型之構圖框架，阻止肢體異常增生。                                   |
| **IP-Adapter**            | 高密度風格遷移器。可強勢提取某張參照圖之視覺特徵、色調或特有筆觸，將同等美學質測套用到新生成之內容上。                               |
| **RIFE (幀插值網路)**     | AI 補幀渲染器。當原始影片模型輸出幀率 (FPS) 過低而導致視覺跳格時，利用演算法推算兩幀之間遺失的過場畫面，平滑放大至 60 FPS 級別水平。 |

---

_熟悉上述底層技術之分類，便可於後續配置 ComfyUI 的節點流程時，胸有成竹地篩選所需之模型組件，打造高效率的內容產製工廠。_
