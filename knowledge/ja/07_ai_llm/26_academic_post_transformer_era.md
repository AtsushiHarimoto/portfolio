# 26. ポスト Transformer 時代の黄昏：MoE、Mamba と空間知能 (AGI 終局)

> **種類**: 最先端 AI (SOTA) 発展フロンティアと学術アーキテクチャ展望
> **重点**: Transformer が AI 界を 8 年間支配してきましたが、ハードウェアの計算力天井とメモリウォールが迫っています。本稿は世界のトップ研究者 (Yann LeCun, 李飛飛) の探索方向を集約し、大規模モデルの次のステップである「力技の分解」と「基盤の再構築」を検討します。**MoE (混合エキスパートモデル / Mixture of Experts)**、**Mamba (状態空間モデル / State Space Models)**、および純粋な言語予測を完全に廃棄することを目指す**ワールドモデル (World Models) / 空間知能 (Spatial Intelligence)** を深く解析します。

---

## 前言：力技で言語を窮め尽くすことは、AGI への唯一の行き止まりなのか？

今日の GPT-4 は賢いですが、本質的には「次のトークンを予測する (Next-Token Prediction)」だけです。
AI の父であり Meta の主席科学者 Yann LeCun は、この自己回帰 (Auto-regressive) 言語モデルを容赦なく批判しました：_「大規模言語モデルは猫より馬鹿だ。猫は目で世界を見ただけで、コップをテーブルの端に押せば落ちることを知っている。LLM はインターネット上で『ニュートンの重力理論』を何十万回も読んだから、『落ちる』という 3 文字を当てずっぽうで出しただけだ。」_

この根本的な知能の欠陥、および Transformer が超長文を読む際にハードウェアをクラッシュさせる「二次関数複雑度 $\mathcal{O}(N^2)$」の不治の病を解決するため、シリコンバレーの野心家たちは 3 方面から革命を起こしています。

---

## 1. 省エネ至上主義の集団偏執狂：混合エキスパートモデル (MoE)

モデルが極限の規模（例：1000 億パラメータ超）に達した時、回答 1 つのたびにニューラルネットワークのすべての細胞がエネルギー消費する演算を発動すれば、Microsoft でさえ破産します。

### 分割統治の MoE (Mixture of Experts)

全知全能のスーパーブレインを 1 つ養えないなら、**「8 人の極端な偏執狂（エキスパート）」** を養えばいい！

- **メカニズム**：Transformer の一部のフィードフォワードネットワーク (FFN) 層で、科学者がそれを水平に切り分け、8 つの独立した回路基板 (Experts) にしました。
- **ルーター (Router)**：ニューラルネットワークの小さな交換台です。入力（例：`print("Hello")`）が来ると、ルーターは一目でプログラミング言語関連だと見抜き、ためらいなくパケットを「第 3 エキスパート（Python 専門）」に送り、他の 7 つのエキスパートを強制的に「電源オフ・バケーション」させます。
- **利点**：モデルは見かけ上驚異的な 80B（800 億）パラメータの容量を持ちますが、トークンを 1 つ出力するたびに、実際に「発電しているニューロン (Active Parameters)」は 10B パラメータのみです。**これにより巨大モデルの知性の幅を維持しつつ、推論コストと消費電力を 1/8 に切り下げます。**

_(今日最も人気のあるオープンソースモデル、例えば Mixtral 8x7B、Qwen1.5 MoE、さらには噂の GPT-4 本体も、すべてこの「組み立て車」アーキテクチャです。)_

---

## 2. 王に挑む毒蛇：Mamba と状態空間モデル (SSM)

第 24 篇で Transformer の神格化の戦いについて述べました。しかしタダ飯はありません：Transformer は「全品食べ放題」で、本を読む際にすべての単語が前のすべての単語とアテンション交換を行います。これによりコンテキスト長が 10 万トークンに達すると、メモリ消費が二乗で爆発します $\mathcal{O}(N^2)$。

### Mamba 革命

この問題に対処するため、スタンフォード大学と研究者グループが古代の線形システム理論を復活させ、極めて衝撃的な新しいネットワークアーキテクチャを開発しました：**SSM (State Space Models / 状態空間モデル)**、そしてその最新かつ最強の変体が **Mamba** です。

- **全知の視点を放棄**：Mamba は集合写真 (Self-Attention) を放棄しました。最後の単語が前の 10 万語をスキャンすることを要求しません。
- **神級の速記者 (Selective State)**：ベテランの聴き取り書き記者のように振る舞います。前文を読む際、「重要なポイント」を選択的に極度に圧縮された脳領域（Hidden State）に記録します。無駄な文に遭遇すると自動的に忘却します（Forget Gate）。
- **線形の暴走**：この「小さなノートブック」だけを持って前方に読み進めるため、10 万語でも 100 万語でも、計算複雑度は常に線形 $\mathcal{O}(N)$、メモリ消費は定数 $\mathcal{O}(1)$ です！超長文の読解速度は Transformer 陣営の 5 倍です！

_(現在のところ Mamba は「複雑な因果推論の理解」の細部で Transformer にわずかに及びませんが、次の 10 年の基盤モデルを席巻する最も有望なアーキテクチャとして広く認識されています。)_

---

## 3. この世界を見る：空間知能と JEPA (World Models)

これは真の汎用人工知能 (AGI) への最も神聖な塔です。
李飛飛 (Fei-Fei Li) と Yann LeCun がこの純粋なテキストを放棄する宗教革命をリードしています。

- **テキストは人類の究極の圧縮器**：「赤いボールが跳ねている」と言います。言語はわずか数個の抽象的シンボルしか使いません。しかし実世界では、光影、材質の反射、重力加速度、変形など数億ピクセル単位の物理的衝突が関わっています。大規模モデルはテキストだけからこれを学ぶことは永遠にできません。
- **ワールドモデル (World Models / V-JEPA)**：LeCun 主導の連合埋め込み予測アーキテクチャ (Joint-Embedding Predictive Architecture)。AI に次の「単語」を予測させるのではなく、数秒間の無音の実世界動画を見せた後、**マップ空間内で動画の次の 1 秒のフレームベクトルと相対的な物理属性を「最初から最後まで」直接予測させます。**
- **空間知能 (Spatial Intelligence)**：李飛飛は知覚 (Seeing) と行動 (Doing) の結合を推進しています。未来の AI（ロボット犬やヒューマノイドロボット）がコップの水を見た時、ニューラルネットワークの脳内でリアルタイムの 3D 構築とグリップ衝突境界シミュレーションを実行します。

**これは、大規模モデルがもはや「超強力な図書館管理員」ではなく、具身知能 (Embodied AI) を獲得し、物理現象を知覚して 3D 空間で生き残ることができる「超人的な家庭教師と実体化バトラー」となることを意味しています。**

---

## Vibecoding 最先端テクノロジー探索ガイド

AI エージェントを使って最新のオープンソースモデルデプロイメントを行う際、または自社の将来の AI トランスフォーメーションロードマップを調査する際：

> `「我が社の 2026 年、あるいは 2027 年の《AGI と大規模モデルアーキテクチャ進化レポート》を執筆する際、Transformer と Attention の行き止まりから脱却してください！重点を以下に置くことを求めます：(1) 我々のサーバーに【Mixture of Experts (MoE)】を導入してパラメータをスパース化 (Sparse) し、秒あたりの推論コストを圧縮すること。(2) 超長文テキスト処理において亜二次関数 (Sub-quadratic) 特性を持つ【Mamba (SSM 状態空間モデル)】を将来の潜在的代替品として調査すること。(3) 将来のコンピュータビジョンまたは具身知能タスクにおいて、【JEPA (連合埋め込み予測)】ベースのワールドモデル (World Models) アーキテクチャの導入を検討し、AI が連続的な高次元の物理および空間知能の課題に対応できるようにすること ── これこそが AGI に向かう正しいレーンです！」`
