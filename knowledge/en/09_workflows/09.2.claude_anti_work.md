# Vibe Coding Automation SOP

## Overview

- For specific feature development or issue investigation, users must first define the **Project Scope** and **Work Mode (RUN-DEV / RUN-QA)**.
- Standard workflows:
  - **Command `RUN-DEV`**: Standard development SOP
    - `New Feature / Bug Fix / Refactoring -> Review/Write Docs -> Code Implementation -> Code Review -> Fix Defects -> Review Approved (Review OK)`
  - **Command `RUN-QA`**: Quality assurance and testing SOP
    - `Understand UT/QA Spec Docs -> Review/Write Test Plan -> Implement Test Engineering -> Code Review -> Fix Defects -> Test Critical Paths -> Produce Test Report`

---

## Project Scope Boundary Control

- Before starting any task, strictly index according to the directory hierarchy below. Even when tasks span multiple subsystems, **explicitly tag the current SCOPE at every step** (e.g., API / moyin-web / moyin-game / tests).
- Hard rules:
  - All operations must be confined to the designated project path. **Cross-boundary modifications are strictly prohibited** (unless the user explicitly authorizes cross-system refactoring).
  - When searching and reading documentation, the system must automatically exclude specific noise directories and never treat them as valid requirement sources.

### Excluded Directory Keywords

The following directories (or folders containing these keywords) are unconditionally excluded as "requirement sources":

- `pending`
- `review`
- `output`
- `LOG`
- `plan`
- `5.QA`
- `5.Q&A`
- `temp`
- `memo`
- `TODO`
- `reiview` (legacy typo also excluded)

> Exception: The `review` directory stores **final output artifacts** (this SOP writes to it), but it must not be used as a "requirement input" source.

---

## Project Module Matrix (Based on Repository Structure)

### Module 1: API Service Layer (Backend API Gateway)

- Source code: `projects/moyin-gateway/web2api`
- Documentation root: `projects/moyin-gateway/docs`
- Review records: `projects/moyin-gateway/docs/review`

### Module 2: Web Application (Moyin Web - Creator Studio)

- Source code: `projects/moyin-web`
- Documentation root: `projects/moyin-web/docs`
- Review records: `projects/moyin-web/docs/review`

### Module 3: Game Frontend Engine (Moyin Game V1)

- Source code: `projects/moyin-game-v1/website`
- Documentation root: `projects/moyin-game-v1/docs/`
- Review records: `projects/moyin-game-v1/docs/review`

### Module 4: QA Automation

- Source code: `projects/tests`
- Documentation root: `projects/tests/docs/`
- Review records: `projects/tests/docs/review`

### Deliverables Archive

- Unified path: `deliverables/`
- Standard output structure:
  - `deliverables/01_要件定義`
  - `deliverables/02_基本設計`
  - `deliverables/04_開発`
  - `deliverables/05_単体テスト`
  - `deliverables/06_結合テスト`
  - `deliverables/07_総合テスト`
  - `deliverables/08_リリース`
  - `deliverables/09_運用保守`

> Note: The deliverables module focuses on high-level documentation for external reviews and cross-team alignment. Engineering specifications remain in `projects/*/docs` as the single source of technical truth.

---

## Universal Workflow Constraints (Apply to Both RUN-DEV and RUN-QA)

- **Language**: Traditional Chinese is mandatory (English technical terms are acceptable), unless a specific workflow overrides this with its own language rules.
- **Logic Pre-check**: Before implementation, verify whether the requirement contains: logical contradictions / technical infeasibility / conflicts with existing documentation.
  - If conflicts are detected: **Report the conflict points and their impact, then proactively ask the user to choose between "revise the spec document" or "override the codebase"**. No large-scale refactoring is permitted without explicit consent.
- **Scope Lock**: All search and output operations are restricted to the assigned project scope.

---

## Core Development SOP (RUN-DEV Flow)

### Step 1: Specification Document Review (Product Manager Perspective)

- **Input**: User provides a feature objective / defect description / designated project scope (if scope is unspecified, the AI must trigger an ASK prompt).
- **Action (Performed by Planning-Layer AI, e.g., Claude)**:
  1. Filter technical specifications and historical constraints from the target system's `docs/` directory.
  2. If discrepancies are found between "spec documents vs. running code," compile a review issue record and archive it at:
     - `{project doc root}/review/issues/` (naming convention: `DocReview-YYYYMMDD.md`)
  3. If the user's requirements are fragmented or internally contradictory, compile a FAQ and output it to:
     - `{project doc root}/review/FAQ/` (naming convention: `FAQ-YYYYMMDD.md`)

### Step 2: Documentation Revision (Doc-First Principle)

- **Action**: The planning-layer AI proposes patches and revised sections for specification documents.
  - Limited to the `docs/` scope of the current project.
  - Revisions must be precise enough to support downstream code implementation and unit verification.
- **Decision Fork**: If the user rejects the documentation revision, the flow resets to Step 1 to realign requirement boundaries.

### Step 3: Code Implementation (Execution Layer - Antigravity/Codex)

- **Action**: The execution-layer AI strictly codes according to the latest approved documentation.
- **Abort Mechanism**: If a logic gap or undefined spec is encountered during implementation:
  - Immediately halt coding. Raise the technical issue to `{project doc root}/review/FAQ/` for the planning-layer AI to resolve.
  - Coding may only resume after the spec is patched (AI must never implement based on speculation).

### Step 4: Code Quality Review (Technical Architect Perspective)

- **Action**: The planning-layer AI scans and reviews:
  - The code diff produced by this task (within quota).
  - Affected core logic modules.
- **Output**: An in-depth review report filed at `{project doc root}/review/issues/` (naming convention: `CodeReview-YYYYMMDD.md`).
- **Risk Classification**:
  - **P0 Blocker**: Will cause core feature failure / data corruption / critical security vulnerability / test blockage.
  - **P1 Critical**: High probability of edge-case bugs / maintainability degradation / state machine logic contradictions.
  - **Risk Warning**: No immediate impact, but structural risk when scaling or integrating with large models.

### Step 5: Packaging and Commit Strategy (Version Control Gate)

- **Action**: Before requesting confirmation, the execution-layer AI must present a complete evidence checklist:
  - Changed files matrix.
  - `git diff` summary.
  - `git log -n 10` trace review.
- **Closure**: Commit incrementally by feature and logical block (ensuring each commit carries clear engineering semantics). If any change is ambiguous, trigger an ASK to re-confirm with the user.

---

## Quality Assurance and Testing SOP (RUN-QA Flow)

### Step 1: Test Documentation Matrix Review (QA Engineer Perspective)

- **Input**: User provides the feature boundary to verify / regression test target / designated project (typically `projects/tests` mounted to the target project).
- **Starting Point**: The planning-layer AI first searches the target project's docs for "testability standards" and "acceptance criteria."
- **Blocking Condition**: If a structural mismatch is found between "test requirements vs. code reality," the test plan must be **forcibly halted**, and a conflict report produced at:
  - `{target project doc root}/review/issues/` or `projects/tests/docs/review/issues/`.

### Step 2: Test Plan Construction

- **Action**: Produce a systematic test design blueprint in `projects/tests/docs/`, covering:
  - Test coverage boundaries (In-Scope) and exemptions (Out-of-Scope).
  - Test case inventory (Unit / E2E / Non-functional).
  - Mock data, preconditions, and expected results (assertions).
  - Standardized crash report structure.
- **Archive**: Filed at `projects/tests/docs/plan/` (naming convention: `TestPlan-YYYYMMDD.md`).

### Step 3: Test Implementation (Execution Layer)

- **Action**: The execution-layer AI writes test scripts in the `projects/tests` directory following the test plan.
- **Unknown Edge Cases**: When ambiguous boundaries are encountered, write to `review/FAQ/` for planning-layer intervention.

### Step 4: Test Script Quality Review (Code Review on Tests)

- **Focus**: Review test scripts in `projects/tests`. Evaluate whether test coverage targets the core risk areas and verify assertion logic rigor.
- **Archive**: Filed at `projects/tests/docs/review/issues/` (naming convention: `TestCodeReview-YYYYMMDD.md`).

### Step 5: Test Closure and Version Commit

- **Output**: Auto-generate a comprehensive test conclusion report at `projects/tests/docs/report/` (naming convention: `TestReport-YYYYMMDD.md`).
- **Version Control**: Same discipline as RUN-DEV Step 5 -- review diff, log, and commit incrementally.

---

## Command Center: Daily Interaction Protocol

- When starting a new task sequence, users must provide three foundational commands:
  1. **Select mode**: `RUN-DEV` or `RUN-QA`.
  2. **Declare scope**: API / moyin-web / moyin-game / tests (multiple selections allowed, but at least one required).
  3. **Define objective**: A one-sentence description of the core task + expected acceptance state (the more specific, the better for AI alignment).
- If prerequisite information is incomplete, the AI must trigger an `ASK` mechanism to gather the missing details before entering Step 1.

---

## FAQ & Troubleshooting

### Q: Why does the AI agent (e.g., Claude) frequently overstep by modifying code without permission? Is it a misconfiguration or an AI comprehension issue?

**A: This over-proactive behavior stems from system-level constraints not being forcibly loaded or lacking sufficient deterrent semantics. It is not an AI intelligence deficit.**

Three common root causes:

1. **Global Rule (System Prompt) Loading Chain Failure**:
   - When creating new conversations, switching working directories, or crossing sessions, the mandatory rules in `.agent/rules` may not be fully read into the current context window.
   - Symptom: The AI smoothly analyzes the architecture and proceeds to produce replacement code blocks, or even executes replacements directly.
2. **Overly Soft Defense Semantics (Soft Guidelines vs. Hard Constraints)**:
   - For example, defining only "please follow PLAN then ACTION" -- the AI interprets this as a "recommended best practice" and exercises its own judgment to enter coding early.
   - Without explicit statements like "Without approval, the system is prohibited from modifying any physical files / producing diffs," the defense fails.
3. **Prompting Bias**:
   - When users phrase commands as "How should I fix this?" or "Please give me a fix," it triggers the AI's completion instinct, bypassing the architecture design phase.

### Standard Defenses and Countermeasures:

To fully tame and lock down AI behavior, apply the following mandatory interventions:

**1. Tactical Level (Conversation Gate)**:
Insert an inviolable command gate at the start of every bug report or new requirement:

> **"Hard constraint: This round only authorizes architecture analysis (PLAN) and root cause investigation. All file overwrites, patch generation, and diff application are prohibited. No code implementation may proceed without my explicit 'OK' reply."**

**2. Strategic Level (Foundation Rule Hardening)**:
In top-level constraint files like `00_core_protocol.md` or `06_documents_first.md`, embed a top-tier protection directive:

> **"Zero-Tolerance Policy: Before receiving explicit user OK authorization, all operations that alter workspace physical file state are prohibited, including but not limited to: automatic source code correction, patch file export, diff command application, and git commit actions."**

**Final Verdict**: AI agents possess excellent operational capabilities. The solution lies in establishing precise, zero-tolerance "decidable hard prohibitions" and enforcing permission gates at the start of every session.
