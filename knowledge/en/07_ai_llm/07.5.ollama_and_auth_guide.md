# 07.5. Ollama Local Model Deployment and Cloud LLM Authentication Integration Guide

> **Type**: Technical Integration Manual and System Operations Guide
> **Date**: 2026-01-12 (Revised v1.2)
> **Scope**: Moyin routing gateway (`web2api`) and frontend integration modules

---

## Summary

This guide establishes the standard operating procedures for two major language model integration strategies within the Moyin project:

1. **Local Hosting**: Covers Ollama server deployment, advanced model selection for different VRAM tiers, and the Native Context conversation-continuation technique that breaks through stateless bottlenecks.
2. **Cloud API Access Authentication**: Presents a practical approach for painlessly hijacking Cookies via a Firefox add-on workflow, thoroughly resolving anti-bot and token-expiration pain points for cloud services like Gemini.

---

## 1. Offline Model Hub: Ollama Deployment Guide

### 1.1 Startup and System Resident Management

- **Installation**: On Windows, it is strongly recommended to use the system-level package manager by running `winget install Ollama.Ollama`, or to download directly from the [official Ollama website](https://ollama.com).
- **Daemon Startup**:
  - On Windows, Ollama resides as a background Windows Service by default at system boot.
  - For manual startup or real-time log viewing, run `ollama serve` in the terminal.
- **Health Check**: Send a GET request to `http://localhost:11434/api/tags` via browser. If the response is a standard JSON model list, the server heartbeat is healthy.
- **Batch Initialization**: To ensure development environment consistency, new team members can directly run `.\projects\moyin-gateway\web2api\scripts\setup-ollama-models.ps1`. The system will automatically poll and pull all required parameter models.

### 1.2 Edge Model Selection Strategy by VRAM

To ensure interactive responsiveness, strictly apply the following quantized models according to the host machine's physical VRAM configuration:

| GPU Memory Threshold | Recommended Model Tier | Practical Scenario and Performance Characteristics |
| :--- | :--- | :--- |
| **Ultra-light (2-4GB)** | `llama3.2:3b`, `phi3:mini` | Blazing-fast inference. Limited to basic API encapsulation testing or low-intelligence NPC scheduling tasks. |
| **Standard (6-8GB)** | `qwen2.5:7b-instruct-q4_K_M` | **Highly recommended**. Excellent Traditional Chinese language feel and well-balanced reasoning. Top choice for laptop dev machines or standalone Galgame engines. |
| **Mid-High (12GB)** | `qwen2.5:14b-instruct` | Deeper reasoning paths. Dedicated to complex scenario analysis involving multiple state changes (State-Delta). |
| **Full Power (24GB+)** | `qwen2.5:32b`, `qwq:32b-preview` | **RTX 3090/4090 exclusive**. Epic-tier long-text memory and Deep Reasoning capabilities. |

---

## 2. Web2API Communication Architecture and Performance Optimization

### 2.1 Abandoning the Compatibility Layer, Embracing the Native Context Protocol

To achieve "millisecond-level response" worthy of a visual novel, we deliberately deprecated the traditional OpenAI compatibility layer and deeply integrated the **Ollama native API (`/api/chat`)**:

- **KV Cache Reuse**: After completing a single inference pass, Ollama returns an integer TOKEN ID array named `context` (a memory snapshot).
- **Context Pinning**: Through the backend's memory lifecycle manager (`cache_manager`), this `context` array is hard-bound to the user's `conversation_id`.
- **Distilled Packet Transmission**: In subsequent conversations, the client (web frontend) only needs to transmit the latest instruction plus a historical `context` slice, eliminating the redundant re-tokenization of lengthy historical text. Computational overhead drops to near zero.

### 2.2 Persistent Persona and Script Mode Clamping

- **Anti-Hallucination Hard Constraints**: Open-source inference models are highly prone to generating unexpected thought fragments (e.g., `<thought>` tags). Strong system prompts must be deployed to forcibly suppress this, constraining the model to output only pure strings parseable by `JSON.parse`.
- **World-View Anchoring**: The protagonist's personality and world-building are constantized as `SYSTEM_INSTRUCTION_JSON`, forcibly anchored every time the `context` is loaded, preventing persona weakening and drift after extended conversations.

---

## 3. Cloud Reverse-Engineered API Cookie Authentication Hijacking

### 3.1 Session Lifespan and Encryption Dilemma

When the project switches to free cloud compute (reverse-accessing Gemini or Grok web interfaces), it must rely on Session Cookies such as `__Secure-1PSID` or `sso_token`. However, these authorization tokens suffer from the following fatal flaws:

- **Short Lifespan**: Model providers frequently rotate tokens without warning, causing authorization failures.
- **OS-Level Deep Encryption**: Chromium's encryption algorithm (v80+) is deeply entangled with the underlying OS (e.g., Windows CryptUnprotectData), making direct Python backend decryption extremely difficult and prone to triggering antivirus alerts.

### 3.2 Breaking Through: Firefox Sandbox Bridge Approach

Testing confirms that using **Mozilla Firefox** as the access bridge delivers the most stable automation experience:

1. **Path Registration**: During initial configuration, the absolute path to the Firefox executable is registered in the hidden file `.firefox_path`.
2. **Automatic Amnesia Recovery**: When starting the intermediary server `run_game_api.ps1`, the system proactively initiates a Cookie validity check.
   - If expired or wiped, the API suspends server startup and invokes Firefox to render the Gemini login interface.
   - The script only resumes after the developer completes credential entry through the browser.
3. **Lossless Extraction**: The `cookie_check.py` script queries the `cookies.sqlite` file established by Firefox directly via SQLite driver. Since Firefox's local storage policy is relatively permissive and does not enforce OS-level key rotation, extraction success rate approaches 100%.

### 3.3 Troubleshooting Checklist

- **Unable to Intercept Token**: Check Firefox's privacy settings and ensure the "Delete cookies and site data when Firefox is closed" option is **not** checked.
- **Path Resolution Error**: If a Firefox upgrade causes path drift, simply delete the `.firefox_path` file. Restarting the gateway script will trigger the re-navigation registration process.
