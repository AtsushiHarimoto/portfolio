# 07.1. AI Galgame Engine LLM API Evolution and Commercialization Strategy

> **Type**: Architecture Evolution and Commercial Planning
> **Focus**: Targeting long-text interactive puzzle-solving and Galgame scenarios, this explores the selection of Large Language Model (LLM) APIs, breaking through memory bottlenecks, and the path to productization.

---

## Core Technical Challenge: Stateless Protocols and Token Explosion

When building interactive visual novels or AI-driven dialog games, developers face the following fundamental challenges:

1. **Official Paid APIs**:
   - **Characteristics**: Based on stateless RESTful protocols.
   - **Challenges**: To ensure the NPC's behavior aligns with the previous context, every single request must carry the entire conversation history and world-building lore. As game time progresses, the Input Tokens will balloon exponentially, leading to out-of-control inference costs and severe network latency.
2. **Web Reverse-engineered APIs**:
   - **Characteristics**: Utilizes the native Session architecture of the web version to maintain stateful memory.
   - **Advantages**: Breaks through token billing limits and supports massive context windows.
   - **Risks**: Extremely unstable, highly susceptible to platform anti-bot blocking (HTTP 403 Forbidden), and entails extremely high long-term maintenance costs.

---

## Game Engine Architecture Roadmap

To achieve the best balance among "operating costs," "player experience," and "system defensive capabilities," the LLM integration strategy of the Moyin game engine is divided into the following three stages:

### v0: State Machine & Rolling Summary

- **Operational Mechanism**: The game engine actively intervenes in memory management. It maintains a set of structured states (`state.json`), paired with a compressed "dynamic rolling summary (`summary`)", and only retains the `recent_dialogue` of the last N=8 rounds to submit to the LLM.
- **Architectural Benefits**: Precisely controls Input Tokens within a constant range, completely eradicating the issue of costs scaling linearly with playtime.
- **Output Specification**: Forcibly requires the LLM to return strictly formatted JSON (which must include `narrative` narration, `choices` branches, and `state_delta` state changes).

### v1: Controller-Writer Pattern Decoupling

- **Operational Mechanism**: Introduces a dual-model collaborative routing setup. A lightweight inference model (Controller) is deployed at the front, while a top-tier model possessing deep emotional empathy (Writer, such as Gemini Pro / Claude 3.5) is hidden at the backend.
- **Division of Responsibilities**: The Controller is exclusively responsible for determining the player's input intent, retrieving and dynamically injecting localized chunks of Lore on demand, and even filtering meaningless dialogue. Finally, it decides whether to trigger the expensive Writer model to draft the plot.
- **Architectural Benefits**: Through funnel-style interception, it massively reduces the invocation frequency and the required context payload of the top-tier inference model.

### v2: Fully Localized Deployment

- **Operational Mechanism**: Deploy open-source 7B~8B class Chinese narrative reasoning models (like Llama-3-8B-Instruct, Qwen-2.5, etc.) directly on the player's machine, conducting highly efficient local inference via `llama.cpp` (in GGUF Q4_K_M quantized format).
- **System Interaction**: The game client comes with a built-in lightweight Local LLM Server, externally providing a serialized interface compatible with OpenAI specifications, achieving 100% offline, disconnected gameplay.
- **Architectural Benefits**:
  - **Zero API Cost**: Completely breaks free from cloud token subscription billing.
  - **Extreme Privacy Protection**: Player conversation data never leaves the border, eliminating compliance review risks.
  - **Ultra-low Latency**: Leveraging the player's consumer-grade GPU (like RTX 30/40 series), achieving millisecond-level streaming generation experiences.
- **Practical Limitations and Solutions**: Local models are restricted by hardware; the context window must be more conservative (N=6, summaries restricted to 200~500 tokens), relying far more heavily on the engine-side `state_delta` JSON for rigorous state transitions.

---

## Commercialization and Operations Strategy

### Stage One: MVP (Minimum Viable Product) Market Validation

- Introduce authorization technologies like "seamless hijacking/injection (Cookie Capture)", allowing players to bind their personal free user Web services. This drastically lowers the game's cloud server operational costs, quickly testing the waters and expanding community traffic.

### Stage Two: BYOK - Bring Your Own Key

- Pivot to a core business model focused on "selling a standalone launcher and the engine kernel." Players must bind the cloud API keys (like Gemini / Claude) they personally apply for on the settings page.
- **Core Selling Point (USP)**: What consumers are essentially buying is the **Prompt framework**, the **closed-loop memory management algorithm**, and the **exclusive plot modules** meticulously refined by the Moyin development team, not the underlying language capabilities themselves.

### Stage Three: Compliance & Disclaimer

- It must be clearly stated in the software's End User License Agreement (EULA): The central nervous system of this service relies on external cloud inference; however, all historical conversation token arrays are only stored on the player's local machine, with absolutely no background uploading or server backups.
- If force majeure connection anomalies occur on the server end, the developer is only responsible for repairing the client-side bridge and holds limited liability regarding restrictions placed by third-party model providers.

---

## ğŸ’¡ Vibecoding Instructions

When asking the AI agent to implement the core architecture of the LLM pipeline for the game engine:

> ğŸ—£ï¸ `"When building the AI core of the Galgame engine, strictly implement the [State Machine & Rolling Summary] pattern. Never pass the entire raw conversation history to the LLM API to avoid explosive token costs! Instead, maintain a concise 'state.json' and a rolling summary of recent dialogue. Also, establish a [Controller-Writer Pattern] to separate cheap intent routing from expensive creative generation!"`
