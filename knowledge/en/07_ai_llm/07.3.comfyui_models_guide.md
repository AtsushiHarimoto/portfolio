# 07.3. ComfyUI Image and Audio Generation Models Purchasing Guide

> **Type**: AI Model Selection and Open Source Component Analysis
> **Focus**: Systematically clarifying the differences and roles between Checkpoints, LoRAs, ControlNets, and multimodal extension suites specifically for ComfyUI, a node-based workflow engine.

---

If the ComfyUI ecosystem is viewed as a highly automated post-production studio for audio and video, then the various model weight files possessing peculiar extensions like `.safetensors` are the different specialized masters hired to work within the studio. Here we outline the domain expertise and physical storage protocols of each module for the Moyin project.

---

## 1. Core Image Inference Engine (Base Generation Models)

Commonly known as the "Large Model" or "Base Model," this is the core functional persona that constructs a picture based on text prompts.

| Model Tier / Architecture       | Technical Characteristics and Advantages                                                                                                                                                                              | Suggested Domain Adaptability                                                                                             |
| :------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------ |
| **SDXL (Balanced Cost-Effect)** | The mainstream diffusion model architecture. It has an excellent open-community ecosystem and moderate demands on consumer-grade graphics card VRAM.                                                                  | Rendering virtual characters, anime-style drawing. Strongly recommend `Animagine XL 4.0` or the 2D-optimized `NoobAI XL`. |
| **Flux (Peak Visual Quality)**  | Next-generation architecture. Can achieve photorealistic light and shadow, and features the rule-breaking evolution of "accurately spelling text in images," but computation memory consumption is extremely massive. | `Flux.1 Dev`: Suitable for commercial-grade photorealistic assets and images requiring typographical spelling.            |

> **üìÅ Directory Deployment Guidelines**: All aforementioned `.safetensors` weights and structural files must be entirely placed within the `ComfyUI/models/checkpoints/` directory.

---

## 2. Dynamic Temporal and Video Generation Models

Models that endow still images with physically simulated fluid dynamics or vitality. Currently within the Moyin workflow ecosystem, we strongly rely on the high-performance video architecture open-sourced by Alibaba: **The WAN 2.2 Series**.

| Task Category   | Operating Principle                                                                                                                                                     |
| :-------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **WAN 2.2 I2V** | Image-to-Video. Takes a single or multiple static images as a seed, deducing and rendering a dynamic video of subsequent frames from it.                                |
| **WAN 2.2 T2V** | Text-to-Video. Purely takes text prompts as input, using diffusion models to conjure an image sequence out of thin air directly across spatial and temporal dimensions. |

> üí° **Hardware Load Alleviation Countermeasures (Quantization)**:
> Video generation model physical files are generally extremely massive (often exceeding 15GB). If you encounter graphics card VRAM overloads or system crashes, prioritize searching the community releases labeled with **`FP8` (8-bit floating point)** or **`GGUF`** for low-precision quantized compressed versions to download.

---

## 3. Domain-Specialized Fine-Tuning Extensions (LoRA - Low-Rank Adaptation)

**Low-Rank Adaptation (LoRA)** is an ultra-lightweight method of model training. By freezing the massive neural network weights of the main model, it inserts a side-branch computation layer to undergo parameter special training using only an extremely small number of samples (like 20 pictures of a specific person or art style).

- **Value**: The produced file is extremely small (around a hundred MBs) and can be freely loaded or hot-swapped. It is used to coerce the AI into accurately locking onto and "imitating" the facial features of an obscure character or applying a specific watercolor/cyberpunk art style when generating images.
- **üìÅ Directory Deployment Guidelines**: Please place them in the `ComfyUI/models/loras/` directory.

---

## 4. Zero-Shot Voice Cloning Engine (Zero-Shot TTS)

Text to Speech modules provide dynamically generated vocal audio for virtual systems. Moyin utilizes the following top-tier open-source solutions:

### üåü GPT-SoVITS

The current absolute overlord of cross-lingual speech synthesis.

- **Core Technology**: Zero-Shot Inference. It only needs a target recording file of 3~5 seconds to act as a Reference Audio track, and the system can parse the vocal print resonance on the spot.
- **Advantages**: Perfect compatibility for mixing Chinese, English, Japanese, Korean, and Cantonese across multiple linguistic barriers. It can also be seamlessly grabbed and mounted from within the ComfyUI Manager.

### üåü IndexTTS2 (Developed by the Bilibili community team)

- **Technical Focus**: Although slightly more difficult to deploy, it performs excellently in "absolute temporal control of the video audio track length (beneficial for later mouth Lip-Sync alignment)" and "high-complexity emotional parsing (supporting the precise placement of tone tags)."

---

## 5. Absolute Visual Precision Control and Auxiliary Nodes (ControlNet & Consistency)

The biggest uncontrollability of large language models lies in the random drift of the imagery. The following auxiliary neural networks (ControlNets) act as powerful spatial or stylistic restrainers:

| Auxiliary Model Type                   | Engineering Purpose and Technical Effect                                                                                                                                                                                                                             |
| :------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ControlNet (OpenPose)**              | Posture skeleton anchor. Feeds special skeleton graphs or stickman sketches to forcibly restrict the compositional framework of the backend diffusion model, preventing anomalous limb growth.                                                                       |
| **IP-Adapter**                         | High-density style transferrer. Can forcefully extract the visual features, color tones, or unique brushstrokes of a reference image, applying equivalent aesthetic textures to newly generated content.                                                             |
| **RIFE (Frame Interpolation Network)** | AI frame-enhancement renderer. When the original video model's output framerate (FPS) is too low causing visual stuttering, it utilizes algorithms to calculate the missing transition frames between two existing frames, smoothly upscaling them to 60 FPS levels. |

---

_Being familiar with the classification of the above underlying technologies will allow you to confidently filter the required model components and build a highly efficient content production factory when configuring ComfyUI node workflows later on._

---

## üí° Vibecoding Instructions

When asking the AI agent to interact with or configure a ComfyUI setup:

> üó£Ô∏è `"When preparing deployment scripts or discussing the ComfyUI rendering pipeline architecture with me, clearly distinguish between Checkpoints (Base Generation Models), LoRAs, and ControlNets. Ensure that all '.safetensors' weights are correctly placed in their respective 'models/checkpoints/' or 'models/loras/' directories. For video generation, recommend the use of [WAN 2.2 I2V/T2V] models and suggest downloading low-precision [FP8] or [GGUF] versions to alleviate VRAM overload issues!"`
