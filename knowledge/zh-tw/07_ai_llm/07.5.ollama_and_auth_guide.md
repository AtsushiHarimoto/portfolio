# 07.5. Ollama 本機模型部署與雲端 LLM 認證整合指南

> **類型**: 技術整合手冊與系統操作指南  
> **日期**: 2026-01-12 (修訂版 v1.2)  
> **適用範圍**: Moyin 路由閘道器 (`web2api`) 與前端介接模組

---

## 摘要

本指南旨在確立 Moyin 專案中兩大語言模型介接策略的標準作業流程：

1. **本機模型託管 (Local Hosting)**：涵蓋 Ollama 伺服器之部署、針對不同顯存 (VRAM) 級距之高階模型選型，以及突破無狀態瓶頸的 Native Context 續聊技術。
2. **雲端 API 存取認證 (Cloud Authentication)**：提出以外掛 Firefox 工作流無痛劫持 Cookie 之實務方案，徹底解決 Gemini 等雲端服務的反爬蟲 (Anti-bot) 與 Token 失效痛點。

---

## 1. 離線模型中樞：Ollama 部署指南

### 1.1 啟動與系統駐留管理

- **環境安裝**: 於 Windows 環境，強烈推薦使用系統級封裝管理器執行 `winget install Ollama.Ollama`，或逕赴 [Ollama 官方網站](https://ollama.com) 下載。
- **守護行程啟動**:
  - Windows 預設會於開機時以背景服務 (Windows Service) 形式駐留。
  - 若需手動掛載或檢視即時日誌，請於終端機執行 `ollama serve`。
- **健康度查核 (Health Check)**: 利用瀏覽器向本機端點 `http://localhost:11434/api/tags` 發送 GET 請求，若回傳標準之 JSON 模型清單，即表示伺服器心跳正常。
- **批次初始化**: 為確保開發環境一致性，新進人員可直接執行 `.\projects\moyin-gateway\web2api\scripts\setup-ollama-models.ps1`，系統將自動輪詢並拉取依賴之所有參數模型。

### 1.2 依據 VRAM 之邊緣模型選型策略

為確保互動反饋之流暢度，請依據宿主機器的實體顯存配置，嚴格套用下述量化模型：

| 顯卡記憶體門檻       | 指定推薦模型層別                 | 實務場景與效能特徵                                                             |
| :------------------- | :------------------------------- | :----------------------------------------------------------------------------- |
| **極限輕量 (2-4GB)** | `llama3.2:3b`, `phi3:mini`       | 推論極速。限用於基礎 API 封裝測試或低智商 NPC 排程任務。                       |
| **標準標配 (6-8GB)** | `qwen2.5:7b-instruct-q4_K_M`     | **強烈推薦**。繁中語感極佳與邏輯均衡，為筆電開發機或 Galgame 單機引擎之首選。  |
| **中高階層 (12GB)**  | `qwen2.5:14b-instruct`           | 推理路徑深化。專責處理具備多重狀態變更 (State-Delta) 之複雜場景解析。          |
| **火力全開 (24GB+)** | `qwen2.5:32b`, `qwq:32b-preview` | **RTX 3090/4090 專武**。具備史詩級長文本記憶與深層 (Deep Reasoning) 推演能力。 |

---

## 2. Web2API 通訊架構與效能最佳化

### 2.1 揚棄相容層，擁抱 Native Context 協定

為追求視覺小說級別的「毫秒級響應」，我們在架構上主動廢棄了傳統的 OpenAI 相容層處理模組，深度整合 **Ollama 原生 API (`/api/chat`)**：

- **KV Cache 複用機制**: Ollama 於單次推論完畢後，會回傳一組名為 `context` 的整數型 TOKEN ID 陣列（記憶體快照）。
- **對話鎖定 (Context Pinning)**: 藉由後端的記憶體生命週期管理器 (`cache_manager`)，將該 `context` 陣列與使用者的 `conversation_id` 強綁定。
- **蒸餾封包傳送**: 於後續對話中，客戶端（網頁端）僅需傳輸最新一句指令與歷史 `context` 切片，免除了冗長的歷史文本重複編碼 (Tokenize) 過程，算力負擔降至趨近於零。

### 2.2 長駐人格與劇本模式鉗制

- **反幻覺之強約束**: 開源推理模型極易產生非預期的思維碎念 (如 `<thought>` 標記)。必須透過強勢的系統提示詞 (System Prompt) 實施降維打擊，強制其僅能吐出可被 `JSON.parse` 吞吐之純淨字串。
- **世界觀錨定**: 將主角性格與世界觀常數化為 `SYSTEM_INSTRUCTION_JSON`，於每次 `context` 載入時強制錨定，防堵長時對話後的人格弱化與偏移。

---

## 3. 雲端逆向 API 之 Cookie 認證劫持

### 3.1 Session 存續期與加密困境

當專案切換至雲端免費算力 (逆向存取 Gemini 或 Grok 網頁端) 時，必須依賴諸如 `__Secure-1PSID` 或 `sso_token` 等 Session Cookies。然而，這些授權權杖具備以下致命缺點：

- **存續期短暫**: 模型供應商經常無預警滾動輪替導致授權失效。
- **OS 級深度加密**: Chromium 核心 (v80+) 之加密演算法與作業系統底層 (如 Windows CryptUnprotectData) 高度纏繞，導致利用 Python 後台直接解密提取極端困難且容易觸發防毒警報。

### 3.2 破局之道：Firefox 沙盒橋接方案

經實測，選用 **Mozilla Firefox** 作為存取橋樑能帶來最穩定的自動化體驗：

1. **路徑寫入**: 初次配置時，將 Firefox 單機執行檔之絕對路徑掛載於隱藏檔 `.firefox_path` 中。
2. **失憶自動修復**: 啟動中介伺服器 `run_game_api.ps1` 時，系統將主動發起 Cookie 有效期偵測。
   - 若逾期或遭抹除，API 將暫止伺服器啟動，並喚起 Firefox 繪製 Gemini 登入介面。
   - 待開發者經由瀏覽器填入憑證完畢，腳本方繼續放行。
3. **無損提取 (Data Extraction)**: 呼叫 `cookie_check.py` 腳本，透過 SQLite 驅動直接叩詢 Firefox 設立之 `cookies.sqlite` 檔案。由於 Firefox 的本機存儲原則相對寬鬆且不強制實施 OS 級輪替金鑰，提取成功率近乎 100%。

### 3.3 異常排除查核清單 (Troubleshooting)

- **無法攔截 Token**: 請入內檢查 Firefox 之隱私權設定，嚴禁勾選「關閉瀏覽器時自動清理 Cookie 與網站資料」。
- **路徑尋址錯誤**: 倘若升級 Firefox 導致路徑偏移，請直接刪除 `.firefox_path` 檔案，重啟閘道器腳本即會觸發重新導航註冊流程。
