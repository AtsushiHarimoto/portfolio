# 36. 機率學的奇蹟：海量數據防禦戰 (Bloom Filter & HyperLogLog)

> **類型**: 系統設計極限資料結構與演算法科普
> **重點**: 顛覆「凡事都要精確」的傳統工程師思維。探討當資料量飆升至數十億級別時，如何依靠「機率性資料結構」犧牲 1% 的精確度，換取千倍、萬倍的記憶體與效能拯救！

---

## 前言：精準計算的毀滅性代價

傳統的軟體工程教導我們，要知道一個帳號是否存在，就是去關聯式資料庫 (如 MySQL) 下達 `SELECT COUNT(*)`。要統計這支 YouTube 影片有多少「不重複觀眾 (Unique Views)」，就是在資料庫建一個 `Set` 陣列，把一億人的 ID 全部塞進去。

**在十億級別的高併發場景下，上述做法等於自殺。**

- 存一億個使用者的 `String ID`，哪怕每個只有 10 Bytes，也需要耗損驚人的 1 GB 記憶體！
- 當駭客每秒發動一萬次查詢，故意輸入明知「不存在的使用者 ID」，快取 (Redis) 會全部 Miss，這些請求會像利刃般穿透快取，直接砍在底層的硬碟資料庫上！這稱為 **「快取穿透 (Cache Penetration)」**。

為此，架構師必須向數學與機率低頭，利用兩個魔王級的兵器來換取生存。

---

## 1. 斬斷快取穿透的盾牌：布隆過濾器 (Bloom Filter)

布隆過濾器 (發明於 1970 年) 是一種空間效率極高的「機率型」過濾器。它**不儲存資料本身**，而是用一條只有 0 與 1 的「超長 Bit 陣列」來打雜湊點。

### 🛡️ 它的鐵律與魔法

- **鐵律 1**：當它跟你說「這個 ID **絕對不存在！**」，那就一定不存在！(100% 正確率，沒有 False Negatives)。這時系統可以直接丟出 `404`，連去後端硬碟問都不用問，完美防禦攻擊。
- **鐵律 2**：當它跟你說「這個 ID **『可能』存在喔！**」，那它有極小的機率 (如 0.1%) 是在騙你 (False Positives)。這時我們才放行這個請求去翻找沈重的資料庫，就算被騙撲空了也無妨，我們已經擋下了另外 99.9% 的無效請求！

這讓 Redis 只需耗費極小幾 Megabytes 的記憶體，就能在毫秒間判定上億筆黑名單或是已註冊的帳號！

### ⚙️ 它是怎麼辦到的？

當我們把 `moyin` 這個帳號存入系統時，布隆過濾器會把 `moyin` 丟進三個不同的雜湊函數 (Hash Function)，算出三個數字 (例如 `2, 53, 91`)，然後把位元陣列的這三個位置從 `0` 塗黑變成 `1`。
當駭客來問 `hacker123` 時，系統去算他的三個位置 (例如 `2, 18, 91`)，只要發現第 18 個位置還是白色的 `0`，這代表這個字串絕對沒有被存過！直接秒殺！
_(但如果小偷運氣極好，他算出來的位置剛好都被別人的點給塗黑了，這就是那 0.1% 的誤判機率)。_

---

## 2. 一億人也只要 1.5 KB 的黑洞：HyperLogLog (HLL)

如果你要統計 YouTube 首頁每支影片的「不重複點閱人數 (Unique Viewers/DAU)」，並即時顯示在大屏幕上。
傳統用 `HashSet` 計算一千支影片的一億訪客，至少需要幾千 GB 的記憶體叢集，且每次新增訪客的 $O(1)$ 插入在巨量下依然沉重。

### 🎩 用丟硬幣來估算人頭

HyperLogLog 是一種令人毛骨悚然的數學奇蹟演算法。
它的核心精神是：「如果我連續丟硬幣，出現『連續 10 次正面』的機率是極小的。所以，如果我在一群人裡面，看到有人能丟出『連續 10 次正面』，那這群人一定非常多！」

- **運作**：HLL 把每個進來的訪客 ID 進行雜湊，轉成一串二進位的 `0101000...`數字。然後 HLL 只記錄所有數字中，**「最長出現了幾個連續的 0 (也就是丟出正面的紀錄)」**。
- **瘋狂的壓縮與妥協**：HLL 根本不記得是誰來過！它只用 **12 KB 甚至 1.5 KB** 的空間，維護了幾百個水桶的最長連續 0 紀錄。然後反推估算：「哇！這桶子的水準這麼高，這支影片起碼來了幾百萬人！」
- **結果**：只要犧牲了 $\approx \mathbf{0.81\% \text{ ~ } 2\%}$ 的微小誤差 (YouTube 觀看數 1,000 萬人跟 1,005 萬人對使用者來說根本沒差)，你能省下 **$1,000,000$ 倍的記憶體**！

_(目前 Redis 原生內建了 HyperLogLog 的 `PFADD` 與 `PFCOUNT` 指令，是架構師統計巨量 Dashboard 的絕對首選！)_

---

## 💡 Vibecoding 工地監工發包訣竅

在使用 AI Agent 處理包含巨量黑名單驗證或高頻不重複次數統計時：

> 🗣️ `「你在撰寫這個處理每日千萬級別的【不重複頁面造訪統計 (Unique Page Views)】API 時，嚴禁你把訪客的 Session ID 全部塞進 Redis 的 Set 集合裡浪費記憶體！我允許 1% 的業務展示誤差。請全面改用 Redis 原生的【HyperLogLog (PFADD/PFCOUNT)】進行海量基數估算 (Cardinality Estimation)。另外，為了防禦登入接口被大量隨機帳號撞庫，請在資料庫前方架設一層記憶體級別的【布隆過濾器 (Bloom Filter)】來攔截快取穿透攻擊！」`
